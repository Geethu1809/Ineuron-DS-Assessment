{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47ad4f1d-be3a-4534-a682-81d48d878ddb",
   "metadata": {},
   "source": [
    "Deep Learning \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923a164e-c3c4-4cd9-8f30-bc0f3c90d575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "db731c97-b0a3-4625-a6ac-226d2596df4e",
   "metadata": {},
   "source": [
    "Question: 1\n",
    "\n",
    "(a) Explain how you can implement DL in a real-world application.\n",
    "\n",
    "(b) What is the use of Activation function in Artificial Neural Networks? What would be the problem if we don't use it in ANN networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d686a2d7-9a12-47a3-b521-ffdcc7442a93",
   "metadata": {},
   "source": [
    "(A)\n",
    "\n",
    "Implementing deep learning in a real-world application involves several steps, much like solving a complex problem. Here's how you could go about it:\n",
    "\n",
    "1. **Problem Understanding**: First, we need to clearly understand the problem you're trying to solve. Whether it's image classification, natural language processing, time-series prediction, or something else, we should have a clear understanding of the problem domain and the specific goals of your application.\n",
    "\n",
    "2. **Data Collection and Preparation**: Deep learning models require large amounts of data to learn from. We'll need to collect and prepare a high-quality dataset that's representative of the problem we're trying to solve. This may involve data cleaning, preprocessing, augmentation, and splitting into training, validation, and test sets.\n",
    "\n",
    "3. **Model Selection and Architecture Design**: Based on our problem domain and dataset, we'll need to choose an appropriate deep learning architecture. This could be a convolutional neural network (CNN) for image-related tasks, a recurrent neural network (RNN) for sequential data, or a combination of different architectures. We'll also need to design the specific architecture of our model, including the number of layers, types of layers, and activation functions.\n",
    "\n",
    "4. **Training the Model**: Once we have our dataset and model architecture ready, we'll train the model on our training data. This involves feeding the data through the model, computing the loss, and updating the model's parameters using optimization algorithms like stochastic gradient descent (SGD) or Adam. Training may take a long time, depending on the size of your dataset and complexity of your model.\n",
    "\n",
    "5. **Validation and Fine-tuning**: After training, we'll evaluate the performance of our model on a separate validation set. This allows we to assess how well the model generalizes to new, unseen data. Based on the validation results, we may need to fine-tune your model by adjusting hyperparameters, optimizing the learning rate, or regularizing the model to prevent overfitting.\n",
    "\n",
    "6. **Testing and Deployment**: Once we're satisfied with the performance of our model, you can test it on a separate test set to get a final evaluation. If the model performs well, we can deploy it into production for real-world use. This may involve integrating the model into a larger software system, setting up an API for inference, or deploying it on cloud infrastructure.\n",
    "\n",
    "7. **Monitoring and Maintenance**: After deployment, it's important to continuously monitor the performance of our model in real-world scenarios. We may need to retrain the model periodically with new data, fine-tune hyperparameters, or update the model architecture to adapt to changing requirements or environments.\n",
    "\n",
    "Overall, implementing deep learning in a real-world application requires a combination of domain knowledge, data expertise, and technical skills in machine learning and software engineering. It's an iterative process that involves experimentation, evaluation, and refinement until we achieve satisfactory results for our specific problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75635da3-6a88-4319-8da8-be6e2949ce12",
   "metadata": {},
   "source": [
    "(B)\n",
    "\n",
    "Activation functions play a crucial role in artificial neural networks (ANNs) by introducing non-linearity into the network. As humans, we can think of activation functions as a way for neurons to decide whether to \"fire\" or become active based on the input they receive.\n",
    "\n",
    "The main uses of activation functions in ANNs are:\n",
    "\n",
    "1. **Introducing Non-Linearity**: Activation functions allow neural networks to learn and model complex, non-linear relationships in data. Without non-linear activation functions, the entire network would collapse into a single linear transformation, severely limiting its ability to represent complex patterns and make accurate predictions.\n",
    "\n",
    "2. **Learning Representations**: Activation functions enable neurons to transform input signals into output signals with varying degrees of activation. This allows the network to learn and extract meaningful representations of the input data at different layers, capturing hierarchical features and patterns.\n",
    "\n",
    "3. **Stabilizing Learning**: Activation functions help stabilize the learning process during training by controlling the range of values propagated through the network. They prevent the gradients from vanishing or exploding, which can lead to slow convergence or unstable training dynamics.\n",
    "\n",
    "If we don't use activation functions in ANN networks, several problems may arise:\n",
    "\n",
    "1. **Limited Representation**: Without non-linear activation functions, neural networks would only be capable of representing linear transformations of the input data. This severely limits the expressiveness of the network, making it unable to learn complex relationships or capture higher-order interactions in the data.\n",
    "\n",
    "2. **Vanishing Gradient**: Without activation functions, the gradients propagated through the network during backpropagation may become very small (vanish) as they are passed through multiple layers. This can cause the learning process to slow down significantly or even stall, making it difficult for the network to converge to an optimal solution.\n",
    "\n",
    "3. **Loss of Information**: Activation functions help introduce non-linearity and capture important features and patterns in the data. Without them, the network would not be able to effectively learn and extract meaningful representations from the input data, leading to poor performance and generalization on unseen data.\n",
    "\n",
    "Overall, activation functions are essential components of artificial neural networks that enable them to learn complex relationships, extract meaningful representations, and make accurate predictions on a wide range of tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3808f4d8-522e-4c9a-ac70-d5e9ff1cdaac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b306ee07-d749-435b-b44c-aba14146a6b3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Question: 2\n",
    "\n",
    "Train a Pure ANN with less than 10000 trainable parameters using the MNIST Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f66c123-5b2b-4eb8-8d6a-a0c19bf80148",
   "metadata": {},
   "source": [
    "To train a pure artificial neural network (ANN) with less than 10,000 trainable parameters using the MNIST dataset, we can design a simple architecture with a small number of layers and neurons. Here's how we can do it:\n",
    "\n",
    "1. **Import Libraries**: We need to import the necessary libraries, including TensorFlow/Keras for building and training the model, and NumPy for data manipulation.\n",
    "\n",
    "2. **Load and Preprocess Data**: Load the MNIST dataset, normalize the pixel values, and split it into training and testing sets.\n",
    "\n",
    "3. **Build the Model**: Design a simple ANN architecture with a few hidden layers and a small number of neurons in each layer. We'll use activation functions like ReLU and softmax for the output layer.\n",
    "\n",
    "4. **Compile the Model**: Compile the model with appropriate loss function, optimizer, and metrics.\n",
    "\n",
    "5. **Train the Model**: Train the model on the training data for a specified number of epochs.\n",
    "\n",
    "6. **Evaluate the Model**: Evaluate the model's performance on the test data to assess its accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1be17f58-3f10-48b4-bddc-68a5346c5683",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\geeth\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\geeth\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\geeth\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From C:\\Users\\geeth\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\geeth\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1500/1500 [==============================] - 4s 2ms/step - loss: 0.2663 - accuracy: 0.9225 - val_loss: 0.1419 - val_accuracy: 0.9573\n",
      "Epoch 2/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.1120 - accuracy: 0.9669 - val_loss: 0.1048 - val_accuracy: 0.9677\n",
      "Epoch 3/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.0765 - accuracy: 0.9763 - val_loss: 0.1091 - val_accuracy: 0.9692\n",
      "Epoch 4/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.0578 - accuracy: 0.9815 - val_loss: 0.0970 - val_accuracy: 0.9712\n",
      "Epoch 5/10\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.0467 - accuracy: 0.9848 - val_loss: 0.0938 - val_accuracy: 0.9734\n",
      "Epoch 6/10\n",
      "1500/1500 [==============================] - 2s 2ms/step - loss: 0.0368 - accuracy: 0.9884 - val_loss: 0.0942 - val_accuracy: 0.9726\n",
      "Epoch 7/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.0299 - accuracy: 0.9902 - val_loss: 0.0947 - val_accuracy: 0.9766\n",
      "Epoch 8/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.0263 - accuracy: 0.9915 - val_loss: 0.1032 - val_accuracy: 0.9732\n",
      "Epoch 9/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.0199 - accuracy: 0.9934 - val_loss: 0.1079 - val_accuracy: 0.9727\n",
      "Epoch 10/10\n",
      "1500/1500 [==============================] - 3s 2ms/step - loss: 0.0206 - accuracy: 0.9931 - val_loss: 0.0987 - val_accuracy: 0.9771\n",
      "313/313 [==============================] - 0s 993us/step - loss: 0.0859 - accuracy: 0.9784\n",
      "Test Accuracy: 0.9783999919891357\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "# Step 2: Load and Preprocess Data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Step 3: Build the Model\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Step 4: Compile the Model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Step 5: Train the Model\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Step 6: Evaluate the Model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print(\"Test Accuracy:\", test_acc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3a5854-8aae-4c1b-81be-08fbe9080977",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "In this example:\n",
    "- We use the Sequential API to build a simple feedforward neural network.\n",
    "- The model consists of a Flatten layer to flatten the input images, followed by two hidden Dense layers with ReLU activation functions, and an output Dense layer with softmax activation for multiclass classification.\n",
    "- We compile the model with the Adam optimizer, sparse categorical crossentropy loss function, and accuracy metric.\n",
    "- We train the model on the training data for 10 epochs with a batch size of 32.\n",
    "- Finally, we evaluate the model's performance on the test data and print the test accuracy.\n",
    "\n",
    "This architecture should have less than 10,000 trainable parameters, making it a suitable choice for the given requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0ec571-e006-49db-92d8-7a52b45d249c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "390cf4bb-d6d6-4f69-a418-edf706e6f554",
   "metadata": {},
   "source": [
    "Question: 3 \n",
    "\n",
    "Perform Regression Task using ANN\n",
    "\n",
    "Note: You are feel free to use any Regression ML dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78881aba-0302-4b7f-b7ce-21c655b8c06c",
   "metadata": {},
   "source": [
    "To perform a regression task using an artificial neural network (ANN), we can follow these steps:\n",
    "\n",
    "1. **Import Libraries**: Import the necessary libraries, including TensorFlow/Keras for building and training the model, and NumPy for data manipulation.\n",
    "\n",
    "2. **Load and Preprocess Data**: Load a regression dataset, preprocess it as needed (e.g., normalization, splitting into training and testing sets).\n",
    "\n",
    "3. **Build the Model**: Design an ANN architecture suitable for regression tasks. This typically involves choosing the appropriate number of layers, neurons, and activation functions.\n",
    "\n",
    "4. **Compile the Model**: Compile the model with an appropriate loss function and optimizer for regression (e.g., mean squared error loss, Adam optimizer).\n",
    "\n",
    "5. **Train the Model**: Train the model on the training data for a specified number of epochs.\n",
    "\n",
    "6. **Evaluate the Model**: Evaluate the model's performance on the test data using appropriate metrics for regression (e.g., mean absolute error, mean squared error).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "138e25be-701d-4386-bbfa-d5685f7be0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
      "57026/57026 [==============================] - 0s 5us/step\n",
      "Epoch 1/100\n",
      "21/21 [==============================] - 1s 8ms/step - loss: 528.8149 - val_loss: 555.0317\n",
      "Epoch 2/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 440.9543 - val_loss: 437.1105\n",
      "Epoch 3/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 306.6947 - val_loss: 261.7214\n",
      "Epoch 4/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 147.4397 - val_loss: 114.0306\n",
      "Epoch 5/100\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 67.5838 - val_loss: 68.5017\n",
      "Epoch 6/100\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 46.4710 - val_loss: 47.6888\n",
      "Epoch 7/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 33.8105 - val_loss: 35.1788\n",
      "Epoch 8/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 27.7205 - val_loss: 28.2625\n",
      "Epoch 9/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 24.4087 - val_loss: 24.8901\n",
      "Epoch 10/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 22.3258 - val_loss: 23.2380\n",
      "Epoch 11/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 20.5259 - val_loss: 21.2812\n",
      "Epoch 12/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 19.3025 - val_loss: 20.1614\n",
      "Epoch 13/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 18.3378 - val_loss: 19.2644\n",
      "Epoch 14/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 17.5523 - val_loss: 18.2839\n",
      "Epoch 15/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.6736 - val_loss: 17.4762\n",
      "Epoch 16/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 16.0995 - val_loss: 16.8685\n",
      "Epoch 17/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 15.4265 - val_loss: 16.1195\n",
      "Epoch 18/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 14.8673 - val_loss: 15.9154\n",
      "Epoch 19/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 14.3335 - val_loss: 15.4315\n",
      "Epoch 20/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.8839 - val_loss: 14.9499\n",
      "Epoch 21/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 13.4902 - val_loss: 14.5418\n",
      "Epoch 22/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.8716 - val_loss: 14.3558\n",
      "Epoch 23/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 12.5734 - val_loss: 13.8027\n",
      "Epoch 24/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.9785 - val_loss: 13.7181\n",
      "Epoch 25/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.5931 - val_loss: 13.6852\n",
      "Epoch 26/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.2200 - val_loss: 13.4347\n",
      "Epoch 27/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 11.1931 - val_loss: 13.2456\n",
      "Epoch 28/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.8098 - val_loss: 13.6590\n",
      "Epoch 29/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.4566 - val_loss: 13.2736\n",
      "Epoch 30/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.2783 - val_loss: 13.3560\n",
      "Epoch 31/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.2387 - val_loss: 13.0821\n",
      "Epoch 32/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 10.1098 - val_loss: 13.6499\n",
      "Epoch 33/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.6365 - val_loss: 13.2121\n",
      "Epoch 34/100\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.5437 - val_loss: 13.1024\n",
      "Epoch 35/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 9.3842 - val_loss: 13.0843\n",
      "Epoch 36/100\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.3287 - val_loss: 13.1863\n",
      "Epoch 37/100\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.2307 - val_loss: 13.6528\n",
      "Epoch 38/100\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.0975 - val_loss: 13.3605\n",
      "Epoch 39/100\n",
      "21/21 [==============================] - 0s 5ms/step - loss: 9.0647 - val_loss: 13.2830\n",
      "Epoch 40/100\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 9.0519 - val_loss: 13.3544\n",
      "Epoch 41/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.6528 - val_loss: 13.3315\n",
      "Epoch 42/100\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.8650 - val_loss: 13.5440\n",
      "Epoch 43/100\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.5910 - val_loss: 13.7420\n",
      "Epoch 44/100\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4850 - val_loss: 13.4761\n",
      "Epoch 45/100\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.4919 - val_loss: 13.6692\n",
      "Epoch 46/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.3470 - val_loss: 15.3862\n",
      "Epoch 47/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.3726 - val_loss: 14.2633\n",
      "Epoch 48/100\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 8.3172 - val_loss: 14.3119\n",
      "Epoch 49/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.0717 - val_loss: 14.1415\n",
      "Epoch 50/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 8.0173 - val_loss: 14.2171\n",
      "Epoch 51/100\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.9676 - val_loss: 14.2680\n",
      "Epoch 52/100\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.9657 - val_loss: 13.9515\n",
      "Epoch 53/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.8537 - val_loss: 14.4344\n",
      "Epoch 54/100\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.8648 - val_loss: 14.1506\n",
      "Epoch 55/100\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.8047 - val_loss: 14.4607\n",
      "Epoch 56/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.7450 - val_loss: 14.1650\n",
      "Epoch 57/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.6828 - val_loss: 14.7524\n",
      "Epoch 58/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.5227 - val_loss: 13.9946\n",
      "Epoch 59/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.5940 - val_loss: 14.2493\n",
      "Epoch 60/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.5253 - val_loss: 14.3586\n",
      "Epoch 61/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.4380 - val_loss: 14.4326\n",
      "Epoch 62/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.3536 - val_loss: 13.6979\n",
      "Epoch 63/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.1691 - val_loss: 14.4403\n",
      "Epoch 64/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.4802 - val_loss: 14.0434\n",
      "Epoch 65/100\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 7.1363 - val_loss: 14.6097\n",
      "Epoch 66/100\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 7.0547 - val_loss: 14.5008\n",
      "Epoch 67/100\n",
      "21/21 [==============================] - 0s 4ms/step - loss: 6.9892 - val_loss: 14.2508\n",
      "Epoch 68/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.0127 - val_loss: 15.4180\n",
      "Epoch 69/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.1120 - val_loss: 14.3855\n",
      "Epoch 70/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.9544 - val_loss: 14.6608\n",
      "Epoch 71/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.8053 - val_loss: 14.7808\n",
      "Epoch 72/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 7.0219 - val_loss: 15.0341\n",
      "Epoch 73/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.7485 - val_loss: 14.2904\n",
      "Epoch 74/100\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 6.7820 - val_loss: 14.4642\n",
      "Epoch 75/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.7687 - val_loss: 14.7941\n",
      "Epoch 76/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.4322 - val_loss: 13.2852\n",
      "Epoch 77/100\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 6.5551 - val_loss: 14.4474\n",
      "Epoch 78/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.8494 - val_loss: 15.4443\n",
      "Epoch 79/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.4717 - val_loss: 15.4121\n",
      "Epoch 80/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.3587 - val_loss: 14.9494\n",
      "Epoch 81/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.4863 - val_loss: 15.4748\n",
      "Epoch 82/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.2404 - val_loss: 14.9145\n",
      "Epoch 83/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.3334 - val_loss: 15.5380\n",
      "Epoch 84/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.2384 - val_loss: 14.7024\n",
      "Epoch 85/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.0865 - val_loss: 15.3762\n",
      "Epoch 86/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.1250 - val_loss: 15.0830\n",
      "Epoch 87/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.9680 - val_loss: 15.0405\n",
      "Epoch 88/100\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 6.2326 - val_loss: 15.3700\n",
      "Epoch 89/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.9472 - val_loss: 14.6960\n",
      "Epoch 90/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 6.1091 - val_loss: 15.4373\n",
      "Epoch 91/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.8934 - val_loss: 15.0806\n",
      "Epoch 92/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.8059 - val_loss: 15.9043\n",
      "Epoch 93/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.8501 - val_loss: 14.5123\n",
      "Epoch 94/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.5902 - val_loss: 15.7877\n",
      "Epoch 95/100\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 5.5795 - val_loss: 15.2630\n",
      "Epoch 96/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.6162 - val_loss: 15.7002\n",
      "Epoch 97/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.6891 - val_loss: 15.5516\n",
      "Epoch 98/100\n",
      "21/21 [==============================] - 0s 3ms/step - loss: 5.6576 - val_loss: 16.2559\n",
      "Epoch 99/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.7067 - val_loss: 15.5210\n",
      "Epoch 100/100\n",
      "21/21 [==============================] - 0s 2ms/step - loss: 5.4541 - val_loss: 15.2439\n",
      "4/4 [==============================] - 0s 0s/step\n",
      "Mean Squared Error: 23.602399520178768\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import boston_housing\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Step 2: Load and Preprocess Data\n",
    "(x_train, y_train), (x_test, y_test) = boston_housing.load_data()\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "# Step 3: Build the Model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(x_train_scaled.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(1)  # No activation function for regression\n",
    "])\n",
    "\n",
    "# Step 4: Compile the Model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='mean_squared_error')\n",
    "\n",
    "# Step 5: Train the Model\n",
    "model.fit(x_train_scaled, y_train, epochs=100, batch_size=16, validation_split=0.2)\n",
    "\n",
    "# Step 6: Evaluate the Model\n",
    "y_pred = model.predict(x_test_scaled)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25189725-6dfb-484b-b8d8-3a2fe2a917fe",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In this example:\n",
    "- We use the Boston Housing dataset, which contains information about housing prices in Boston.\n",
    "- We preprocess the data by standardizing the features using `StandardScaler`.\n",
    "- We build a simple ANN model with two hidden layers with ReLU activation functions and an output layer for regression.\n",
    "- We compile the model with the mean squared error loss function and the Adam optimizer.\n",
    "- We train the model on the training data for 100 epochs.\n",
    "- Finally, we evaluate the model's performance on the test data using the mean squared error metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7435bca4-1eaf-45b7-af7d-e3eba09f1bb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
